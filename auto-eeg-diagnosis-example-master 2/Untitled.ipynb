{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step for main layer... 0\n",
      "step for main layer... 130\n",
      "step for main layer... 260\n",
      "step for main layer... 390\n",
      "step for main layer... 520\n",
      "generating additional patches for the top end of the return sequence...\n",
      "step for top layer... 0\n",
      "step for top layer... 10\n",
      "step for top layer... 20\n",
      "step for top layer... 30\n",
      "step for top layer... 40\n",
      "step for top layer... 50\n",
      "step for top layer... 60\n",
      "step for top layer... 70\n",
      "step for top layer... 80\n",
      "step for top layer... 90\n",
      "step for top layer... 100\n",
      "step for top layer... 110\n",
      "step for top layer... 120\n",
      "step for top layer... 130\n",
      "step for top layer... 140\n",
      "step for top layer... 150\n",
      "step for top layer... 160\n",
      "step for top layer... 170\n",
      "step for top layer... 180\n",
      "step for top layer... 190\n",
      "step for top layer... 200\n",
      "step for top layer... 210\n",
      "step for top layer... 220\n",
      "step for top layer... 230\n",
      "step for top layer... 240\n",
      "step for top layer... 250\n",
      "step for top layer... 260\n",
      "step for top layer... 270\n",
      "step for top layer... 280\n",
      "step for top layer... 290\n",
      "step for top layer... 300\n",
      "step for top layer... 310\n",
      "step for top layer... 320\n",
      "step for top layer... 330\n",
      "step for top layer... 340\n",
      "step for top layer... 350\n",
      "step for top layer... 360\n",
      "step for top layer... 370\n",
      "step for top layer... 380\n",
      "step for top layer... 390\n",
      "step for top layer... 400\n",
      "step for top layer... 410\n",
      "step for top layer... 420\n",
      "step for top layer... 430\n",
      "step for top layer... 440\n",
      "step for top layer... 450\n",
      "step for top layer... 460\n",
      "step for top layer... 470\n",
      "step for top layer... 480\n",
      "step for top layer... 490\n",
      "step for top layer... 500\n",
      "step for top layer... 510\n",
      "step for top layer... 520\n",
      "step for top layer... 530\n",
      "step for top layer... 540\n",
      "step for top layer... 550\n",
      "step for top layer... 560\n",
      "step for top layer... 570\n",
      "step for top layer... 580\n",
      "step for top layer... 590\n",
      "working with horizontal regular layer...\n",
      "y.shape (?, 598, 30)\n",
      "y Tensor(\"patchy_layer_returnfullseq_155/transpose:0\", shape=(598, 30, ?), dtype=float32)\n",
      "M Tensor(\"patchy_layer_returnfullseq_155/GatherNd:0\", shape=(46, 60, 120, ?), dtype=float32)\n",
      "reshaped_nonzero_values Tensor(\"patchy_layer_returnfullseq_155/transpose_1:0\", shape=(?, 46, 60, 120), dtype=float32)\n",
      "PATCH_tensor1 after Tensor(\"patchy_layer_returnfullseq_155/transpose_1:0\", shape=(?, 46, 60, 120), dtype=float32)\n",
      "MPI first:: Tensor(\"patchy_layer_returnfullseq_155/Mul:0\", shape=(?, 46, 60, 120), dtype=float32)\n",
      "MPI before reansversal Tensor(\"patchy_layer_returnfullseq_155/leaky_re_lu_728/LeakyRelu:0\", shape=(?, 46, 60), dtype=float32)\n",
      "self.add_residual True\n",
      "self.nb_sequences -1\n",
      "MPI after reshape Tensor(\"patchy_layer_returnfullseq_155/leaky_re_lu_728/LeakyRelu:0\", shape=(?, 46, 60), dtype=float32)\n",
      "self.vector_size 598\n",
      "MPI after repeating Tensor(\"patchy_layer_returnfullseq_155/repeat/Reshape:0\", shape=(?, ?, ?), dtype=float32)\n",
      "y Tensor(\"patchy_layer_returnfullseq_155/transpose_2:0\", shape=(598, 30, ?), dtype=float32)\n",
      "M Tensor(\"patchy_layer_returnfullseq_155/GatherNd_1:0\", shape=(598, 9, 120, ?), dtype=float32)\n",
      "reshaped_nonzero_values Tensor(\"patchy_layer_returnfullseq_155/transpose_3:0\", shape=(?, 598, 9, 120), dtype=float32)\n",
      "PATCH_tensor_ontop Tensor(\"patchy_layer_returnfullseq_155/transpose_3:0\", shape=(?, 598, 9, 120), dtype=float32)\n",
      "W_MULT_ontop Tensor(\"patchy_layer_returnfullseq_155/Tile_2:0\", shape=(?, 598, 9, 120), dtype=float32)\n",
      "MPI_ontop Tensor(\"patchy_layer_returnfullseq_155/Mul_1:0\", shape=(?, 598, 9, 120), dtype=float32)\n",
      "MPI_ontop at end Tensor(\"patchy_layer_returnfullseq_155/leaky_re_lu_729/LeakyRelu:0\", shape=(?, 598, 9), dtype=float32)\n",
      "MPI after concat Tensor(\"patchy_layer_returnfullseq_155/concat:0\", shape=(?, 598, ?), dtype=float32)\n",
      "step for main layer... 0\n",
      "step for main layer... 130\n",
      "step for main layer... 260\n",
      "step for main layer... 390\n",
      "step for main layer... 520\n",
      "generating additional patches for the top end of the return sequence...\n",
      "step for top layer... 0\n",
      "step for top layer... 10\n",
      "step for top layer... 20\n",
      "step for top layer... 30\n",
      "step for top layer... 40\n",
      "step for top layer... 50\n",
      "step for top layer... 60\n",
      "step for top layer... 70\n",
      "step for top layer... 80\n",
      "step for top layer... 90\n",
      "step for top layer... 100\n",
      "step for top layer... 110\n",
      "step for top layer... 120\n",
      "step for top layer... 130\n",
      "step for top layer... 140\n",
      "step for top layer... 150\n",
      "step for top layer... 160\n",
      "step for top layer... 170\n",
      "step for top layer... 180\n",
      "step for top layer... 190\n",
      "step for top layer... 200\n",
      "step for top layer... 210\n",
      "step for top layer... 220\n",
      "step for top layer... 230\n",
      "step for top layer... 240\n",
      "step for top layer... 250\n",
      "step for top layer... 260\n",
      "step for top layer... 270\n",
      "step for top layer... 280\n",
      "step for top layer... 290\n",
      "step for top layer... 300\n",
      "step for top layer... 310\n",
      "step for top layer... 320\n",
      "step for top layer... 330\n",
      "step for top layer... 340\n",
      "step for top layer... 350\n",
      "step for top layer... 360\n",
      "step for top layer... 370\n",
      "step for top layer... 380\n",
      "step for top layer... 390\n",
      "step for top layer... 400\n",
      "step for top layer... 410\n",
      "step for top layer... 420\n",
      "step for top layer... 430\n",
      "step for top layer... 440\n",
      "step for top layer... 450\n",
      "step for top layer... 460\n",
      "step for top layer... 470\n",
      "step for top layer... 480\n",
      "step for top layer... 490\n",
      "step for top layer... 500\n",
      "step for top layer... 510\n",
      "step for top layer... 520\n",
      "step for top layer... 530\n",
      "step for top layer... 540\n",
      "step for top layer... 550\n",
      "step for top layer... 560\n",
      "step for top layer... 570\n",
      "step for top layer... 580\n",
      "step for top layer... 590\n",
      "working with horizontal regular layer...\n",
      "y.shape (?, 598, 30)\n",
      "y Tensor(\"patchy_layer_returnfullseq_156/transpose:0\", shape=(598, 30, ?), dtype=float32)\n",
      "M Tensor(\"patchy_layer_returnfullseq_156/GatherNd:0\", shape=(46, 60, 120, ?), dtype=float32)\n",
      "reshaped_nonzero_values Tensor(\"patchy_layer_returnfullseq_156/transpose_1:0\", shape=(?, 46, 60, 120), dtype=float32)\n",
      "PATCH_tensor1 after Tensor(\"patchy_layer_returnfullseq_156/transpose_1:0\", shape=(?, 46, 60, 120), dtype=float32)\n",
      "MPI first:: Tensor(\"patchy_layer_returnfullseq_156/Mul:0\", shape=(?, 46, 60, 120), dtype=float32)\n",
      "MPI before reansversal Tensor(\"patchy_layer_returnfullseq_156/leaky_re_lu_733/LeakyRelu:0\", shape=(?, 46, 60), dtype=float32)\n",
      "self.add_residual True\n",
      "self.nb_sequences -1\n",
      "MPI after reshape Tensor(\"patchy_layer_returnfullseq_156/leaky_re_lu_733/LeakyRelu:0\", shape=(?, 46, 60), dtype=float32)\n",
      "self.vector_size 598\n",
      "MPI after repeating Tensor(\"patchy_layer_returnfullseq_156/repeat/Reshape:0\", shape=(?, ?, ?), dtype=float32)\n",
      "y Tensor(\"patchy_layer_returnfullseq_156/transpose_2:0\", shape=(598, 30, ?), dtype=float32)\n",
      "M Tensor(\"patchy_layer_returnfullseq_156/GatherNd_1:0\", shape=(598, 9, 120, ?), dtype=float32)\n",
      "reshaped_nonzero_values Tensor(\"patchy_layer_returnfullseq_156/transpose_3:0\", shape=(?, 598, 9, 120), dtype=float32)\n",
      "PATCH_tensor_ontop Tensor(\"patchy_layer_returnfullseq_156/transpose_3:0\", shape=(?, 598, 9, 120), dtype=float32)\n",
      "W_MULT_ontop Tensor(\"patchy_layer_returnfullseq_156/Tile_2:0\", shape=(?, 598, 9, 120), dtype=float32)\n",
      "MPI_ontop Tensor(\"patchy_layer_returnfullseq_156/Mul_1:0\", shape=(?, 598, 9, 120), dtype=float32)\n",
      "MPI_ontop at end Tensor(\"patchy_layer_returnfullseq_156/leaky_re_lu_734/LeakyRelu:0\", shape=(?, 598, 9), dtype=float32)\n",
      "MPI after concat Tensor(\"patchy_layer_returnfullseq_156/concat:0\", shape=(?, 598, ?), dtype=float32)\n",
      "step for main layer... 0\n",
      "step for main layer... 130\n",
      "step for main layer... 260\n",
      "step for main layer... 390\n",
      "step for main layer... 520\n",
      "generating additional patches for the top end of the return sequence...\n",
      "step for top layer... 0\n",
      "step for top layer... 10\n",
      "step for top layer... 20\n",
      "step for top layer... 30\n",
      "step for top layer... 40\n",
      "step for top layer... 50\n",
      "step for top layer... 60\n",
      "step for top layer... 70\n",
      "step for top layer... 80\n",
      "step for top layer... 90\n",
      "step for top layer... 100\n",
      "step for top layer... 110\n",
      "step for top layer... 120\n",
      "step for top layer... 130\n",
      "step for top layer... 140\n",
      "step for top layer... 150\n",
      "step for top layer... 160\n",
      "step for top layer... 170\n",
      "step for top layer... 180\n",
      "step for top layer... 190\n",
      "step for top layer... 200\n",
      "step for top layer... 210\n",
      "step for top layer... 220\n",
      "step for top layer... 230\n",
      "step for top layer... 240\n",
      "step for top layer... 250\n",
      "step for top layer... 260\n",
      "step for top layer... 270\n",
      "step for top layer... 280\n",
      "step for top layer... 290\n",
      "step for top layer... 300\n",
      "step for top layer... 310\n",
      "step for top layer... 320\n",
      "step for top layer... 330\n",
      "step for top layer... 340\n",
      "step for top layer... 350\n",
      "step for top layer... 360\n",
      "step for top layer... 370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step for top layer... 380\n",
      "step for top layer... 390\n",
      "step for top layer... 400\n",
      "step for top layer... 410\n",
      "step for top layer... 420\n",
      "step for top layer... 430\n",
      "step for top layer... 440\n",
      "step for top layer... 450\n",
      "step for top layer... 460\n",
      "step for top layer... 470\n",
      "step for top layer... 480\n",
      "step for top layer... 490\n",
      "step for top layer... 500\n",
      "step for top layer... 510\n",
      "step for top layer... 520\n",
      "step for top layer... 530\n",
      "step for top layer... 540\n",
      "step for top layer... 550\n",
      "step for top layer... 560\n",
      "step for top layer... 570\n",
      "step for top layer... 580\n",
      "step for top layer... 590\n",
      "working with horizontal regular layer...\n",
      "y.shape (?, 598, 30)\n",
      "y Tensor(\"patchy_layer_returnfullseq_157/transpose:0\", shape=(598, 30, ?), dtype=float32)\n",
      "M Tensor(\"patchy_layer_returnfullseq_157/GatherNd:0\", shape=(46, 60, 120, ?), dtype=float32)\n",
      "reshaped_nonzero_values Tensor(\"patchy_layer_returnfullseq_157/transpose_1:0\", shape=(?, 46, 60, 120), dtype=float32)\n",
      "PATCH_tensor1 after Tensor(\"patchy_layer_returnfullseq_157/transpose_1:0\", shape=(?, 46, 60, 120), dtype=float32)\n",
      "MPI first:: Tensor(\"patchy_layer_returnfullseq_157/Mul:0\", shape=(?, 46, 60, 120), dtype=float32)\n",
      "MPI before reansversal Tensor(\"patchy_layer_returnfullseq_157/leaky_re_lu_738/LeakyRelu:0\", shape=(?, 46, 60), dtype=float32)\n",
      "self.add_residual True\n",
      "self.nb_sequences -1\n",
      "MPI after reshape Tensor(\"patchy_layer_returnfullseq_157/leaky_re_lu_738/LeakyRelu:0\", shape=(?, 46, 60), dtype=float32)\n",
      "self.vector_size 598\n",
      "MPI after repeating Tensor(\"patchy_layer_returnfullseq_157/repeat/Reshape:0\", shape=(?, ?, ?), dtype=float32)\n",
      "y Tensor(\"patchy_layer_returnfullseq_157/transpose_2:0\", shape=(598, 30, ?), dtype=float32)\n",
      "M Tensor(\"patchy_layer_returnfullseq_157/GatherNd_1:0\", shape=(598, 9, 120, ?), dtype=float32)\n",
      "reshaped_nonzero_values Tensor(\"patchy_layer_returnfullseq_157/transpose_3:0\", shape=(?, 598, 9, 120), dtype=float32)\n",
      "PATCH_tensor_ontop Tensor(\"patchy_layer_returnfullseq_157/transpose_3:0\", shape=(?, 598, 9, 120), dtype=float32)\n",
      "W_MULT_ontop Tensor(\"patchy_layer_returnfullseq_157/Tile_2:0\", shape=(?, 598, 9, 120), dtype=float32)\n",
      "MPI_ontop Tensor(\"patchy_layer_returnfullseq_157/Mul_1:0\", shape=(?, 598, 9, 120), dtype=float32)\n",
      "MPI_ontop at end Tensor(\"patchy_layer_returnfullseq_157/leaky_re_lu_739/LeakyRelu:0\", shape=(?, 598, 9), dtype=float32)\n",
      "MPI after concat Tensor(\"patchy_layer_returnfullseq_157/concat:0\", shape=(?, 598, ?), dtype=float32)\n",
      "step for main layer... 0\n",
      "step for main layer... 130\n",
      "step for main layer... 260\n",
      "step for main layer... 390\n",
      "step for main layer... 520\n",
      "generating additional patches for the top end of the return sequence...\n",
      "step for top layer... 0\n",
      "step for top layer... 10\n",
      "step for top layer... 20\n",
      "step for top layer... 30\n",
      "step for top layer... 40\n",
      "step for top layer... 50\n",
      "step for top layer... 60\n",
      "step for top layer... 70\n",
      "step for top layer... 80\n",
      "step for top layer... 90\n",
      "step for top layer... 100\n",
      "step for top layer... 110\n",
      "step for top layer... 120\n",
      "step for top layer... 130\n",
      "step for top layer... 140\n",
      "step for top layer... 150\n",
      "step for top layer... 160\n",
      "step for top layer... 170\n",
      "step for top layer... 180\n",
      "step for top layer... 190\n",
      "step for top layer... 200\n",
      "step for top layer... 210\n",
      "step for top layer... 220\n",
      "step for top layer... 230\n",
      "step for top layer... 240\n",
      "step for top layer... 250\n",
      "step for top layer... 260\n",
      "step for top layer... 270\n",
      "step for top layer... 280\n",
      "step for top layer... 290\n",
      "step for top layer... 300\n",
      "step for top layer... 310\n",
      "step for top layer... 320\n",
      "step for top layer... 330\n",
      "step for top layer... 340\n",
      "step for top layer... 350\n",
      "step for top layer... 360\n",
      "step for top layer... 370\n",
      "step for top layer... 380\n",
      "step for top layer... 390\n",
      "step for top layer... 400\n",
      "step for top layer... 410\n",
      "step for top layer... 420\n",
      "step for top layer... 430\n",
      "step for top layer... 440\n",
      "step for top layer... 450\n",
      "step for top layer... 460\n",
      "step for top layer... 470\n",
      "step for top layer... 480\n",
      "step for top layer... 490\n",
      "step for top layer... 500\n",
      "step for top layer... 510\n",
      "step for top layer... 520\n",
      "step for top layer... 530\n",
      "step for top layer... 540\n",
      "step for top layer... 550\n",
      "step for top layer... 560\n",
      "step for top layer... 570\n",
      "step for top layer... 580\n",
      "step for top layer... 590\n",
      "working with horizontal regular layer...\n",
      "y.shape (?, 598, 30)\n",
      "y Tensor(\"patchy_layer_returnfullseq_158/transpose:0\", shape=(598, 30, ?), dtype=float32)\n",
      "M Tensor(\"patchy_layer_returnfullseq_158/GatherNd:0\", shape=(46, 60, 120, ?), dtype=float32)\n",
      "reshaped_nonzero_values Tensor(\"patchy_layer_returnfullseq_158/transpose_1:0\", shape=(?, 46, 60, 120), dtype=float32)\n",
      "PATCH_tensor1 after Tensor(\"patchy_layer_returnfullseq_158/transpose_1:0\", shape=(?, 46, 60, 120), dtype=float32)\n",
      "MPI first:: Tensor(\"patchy_layer_returnfullseq_158/Mul:0\", shape=(?, 46, 60, 120), dtype=float32)\n",
      "MPI before reansversal Tensor(\"patchy_layer_returnfullseq_158/leaky_re_lu_743/LeakyRelu:0\", shape=(?, 46, 60), dtype=float32)\n",
      "self.add_residual True\n",
      "self.nb_sequences -1\n",
      "MPI after reshape Tensor(\"patchy_layer_returnfullseq_158/leaky_re_lu_743/LeakyRelu:0\", shape=(?, 46, 60), dtype=float32)\n",
      "self.vector_size 598\n",
      "MPI after repeating Tensor(\"patchy_layer_returnfullseq_158/repeat/Reshape:0\", shape=(?, ?, ?), dtype=float32)\n",
      "y Tensor(\"patchy_layer_returnfullseq_158/transpose_2:0\", shape=(598, 30, ?), dtype=float32)\n",
      "M Tensor(\"patchy_layer_returnfullseq_158/GatherNd_1:0\", shape=(598, 9, 120, ?), dtype=float32)\n",
      "reshaped_nonzero_values Tensor(\"patchy_layer_returnfullseq_158/transpose_3:0\", shape=(?, 598, 9, 120), dtype=float32)\n",
      "PATCH_tensor_ontop Tensor(\"patchy_layer_returnfullseq_158/transpose_3:0\", shape=(?, 598, 9, 120), dtype=float32)\n",
      "W_MULT_ontop Tensor(\"patchy_layer_returnfullseq_158/Tile_2:0\", shape=(?, 598, 9, 120), dtype=float32)\n",
      "MPI_ontop Tensor(\"patchy_layer_returnfullseq_158/Mul_1:0\", shape=(?, 598, 9, 120), dtype=float32)\n",
      "MPI_ontop at end Tensor(\"patchy_layer_returnfullseq_158/leaky_re_lu_744/LeakyRelu:0\", shape=(?, 598, 9), dtype=float32)\n",
      "MPI after concat Tensor(\"patchy_layer_returnfullseq_158/concat:0\", shape=(?, 598, ?), dtype=float32)\n",
      "step for main layer... 0\n",
      "step for main layer... 130\n",
      "step for main layer... 260\n",
      "step for main layer... 390\n",
      "step for main layer... 520\n",
      "generating additional patches for the top end of the return sequence...\n",
      "step for top layer... 0\n",
      "step for top layer... 10\n",
      "step for top layer... 20\n",
      "step for top layer... 30\n",
      "step for top layer... 40\n",
      "step for top layer... 50\n",
      "step for top layer... 60\n",
      "step for top layer... 70\n",
      "step for top layer... 80\n",
      "step for top layer... 90\n",
      "step for top layer... 100\n",
      "step for top layer... 110\n",
      "step for top layer... 120\n",
      "step for top layer... 130\n",
      "step for top layer... 140\n",
      "step for top layer... 150\n",
      "step for top layer... 160\n",
      "step for top layer... 170\n",
      "step for top layer... 180\n",
      "step for top layer... 190\n",
      "step for top layer... 200\n",
      "step for top layer... 210\n",
      "step for top layer... 220\n",
      "step for top layer... 230\n",
      "step for top layer... 240\n",
      "step for top layer... 250\n",
      "step for top layer... 260\n",
      "step for top layer... 270\n",
      "step for top layer... 280\n",
      "step for top layer... 290\n",
      "step for top layer... 300\n",
      "step for top layer... 310\n",
      "step for top layer... 320\n",
      "step for top layer... 330\n",
      "step for top layer... 340\n",
      "step for top layer... 350\n",
      "step for top layer... 360\n",
      "step for top layer... 370\n",
      "step for top layer... 380\n",
      "step for top layer... 390\n",
      "step for top layer... 400\n",
      "step for top layer... 410\n",
      "step for top layer... 420\n",
      "step for top layer... 430\n",
      "step for top layer... 440\n",
      "step for top layer... 450\n",
      "step for top layer... 460\n",
      "step for top layer... 470\n",
      "step for top layer... 480\n",
      "step for top layer... 490\n",
      "step for top layer... 500\n",
      "step for top layer... 510\n",
      "step for top layer... 520\n",
      "step for top layer... 530\n",
      "step for top layer... 540\n",
      "step for top layer... 550\n",
      "step for top layer... 560\n",
      "step for top layer... 570\n",
      "step for top layer... 580\n",
      "step for top layer... 590\n",
      "working with horizontal regular layer...\n",
      "y.shape (?, 598, 30)\n",
      "y Tensor(\"patchy_layer_returnfullseq_159/transpose:0\", shape=(598, 30, ?), dtype=float32)\n",
      "M Tensor(\"patchy_layer_returnfullseq_159/GatherNd:0\", shape=(46, 60, 120, ?), dtype=float32)\n",
      "reshaped_nonzero_values Tensor(\"patchy_layer_returnfullseq_159/transpose_1:0\", shape=(?, 46, 60, 120), dtype=float32)\n",
      "PATCH_tensor1 after Tensor(\"patchy_layer_returnfullseq_159/transpose_1:0\", shape=(?, 46, 60, 120), dtype=float32)\n",
      "MPI first:: Tensor(\"patchy_layer_returnfullseq_159/Mul:0\", shape=(?, 46, 60, 120), dtype=float32)\n",
      "MPI before reansversal Tensor(\"patchy_layer_returnfullseq_159/leaky_re_lu_748/LeakyRelu:0\", shape=(?, 46, 60), dtype=float32)\n",
      "self.add_residual True\n",
      "self.nb_sequences -1\n",
      "MPI after reshape Tensor(\"patchy_layer_returnfullseq_159/leaky_re_lu_748/LeakyRelu:0\", shape=(?, 46, 60), dtype=float32)\n",
      "self.vector_size 598\n",
      "MPI after repeating Tensor(\"patchy_layer_returnfullseq_159/repeat/Reshape:0\", shape=(?, ?, ?), dtype=float32)\n",
      "y Tensor(\"patchy_layer_returnfullseq_159/transpose_2:0\", shape=(598, 30, ?), dtype=float32)\n",
      "M Tensor(\"patchy_layer_returnfullseq_159/GatherNd_1:0\", shape=(598, 9, 120, ?), dtype=float32)\n",
      "reshaped_nonzero_values Tensor(\"patchy_layer_returnfullseq_159/transpose_3:0\", shape=(?, 598, 9, 120), dtype=float32)\n",
      "PATCH_tensor_ontop Tensor(\"patchy_layer_returnfullseq_159/transpose_3:0\", shape=(?, 598, 9, 120), dtype=float32)\n",
      "W_MULT_ontop Tensor(\"patchy_layer_returnfullseq_159/Tile_2:0\", shape=(?, 598, 9, 120), dtype=float32)\n",
      "MPI_ontop Tensor(\"patchy_layer_returnfullseq_159/Mul_1:0\", shape=(?, 598, 9, 120), dtype=float32)\n",
      "MPI_ontop at end Tensor(\"patchy_layer_returnfullseq_159/leaky_re_lu_749/LeakyRelu:0\", shape=(?, 598, 9), dtype=float32)\n",
      "MPI after concat Tensor(\"patchy_layer_returnfullseq_159/concat:0\", shape=(?, 598, ?), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step for main layer... 0\n",
      "step for main layer... 130\n",
      "step for main layer... 260\n",
      "step for main layer... 390\n",
      "step for main layer... 520\n",
      "generating additional patches for the top end of the return sequence...\n",
      "step for top layer... 0\n",
      "step for top layer... 10\n",
      "step for top layer... 20\n",
      "step for top layer... 30\n",
      "step for top layer... 40\n",
      "step for top layer... 50\n",
      "step for top layer... 60\n",
      "step for top layer... 70\n",
      "step for top layer... 80\n",
      "step for top layer... 90\n",
      "step for top layer... 100\n",
      "step for top layer... 110\n",
      "step for top layer... 120\n",
      "step for top layer... 130\n",
      "step for top layer... 140\n",
      "step for top layer... 150\n",
      "step for top layer... 160\n",
      "step for top layer... 170\n",
      "step for top layer... 180\n",
      "step for top layer... 190\n",
      "step for top layer... 200\n",
      "step for top layer... 210\n",
      "step for top layer... 220\n",
      "step for top layer... 230\n",
      "step for top layer... 240\n",
      "step for top layer... 250\n",
      "step for top layer... 260\n",
      "step for top layer... 270\n",
      "step for top layer... 280\n",
      "step for top layer... 290\n",
      "step for top layer... 300\n",
      "step for top layer... 310\n",
      "step for top layer... 320\n",
      "step for top layer... 330\n",
      "step for top layer... 340\n",
      "step for top layer... 350\n",
      "step for top layer... 360\n",
      "step for top layer... 370\n",
      "step for top layer... 380\n",
      "step for top layer... 390\n",
      "step for top layer... 400\n",
      "step for top layer... 410\n",
      "step for top layer... 420\n",
      "step for top layer... 430\n",
      "step for top layer... 440\n",
      "step for top layer... 450\n",
      "step for top layer... 460\n",
      "step for top layer... 470\n",
      "step for top layer... 480\n",
      "step for top layer... 490\n",
      "step for top layer... 500\n",
      "step for top layer... 510\n",
      "step for top layer... 520\n",
      "step for top layer... 530\n",
      "step for top layer... 540\n",
      "step for top layer... 550\n",
      "step for top layer... 560\n",
      "step for top layer... 570\n",
      "step for top layer... 580\n",
      "step for top layer... 590\n",
      "working with horizontal regular layer...\n",
      "y.shape (?, 598, 30)\n",
      "y Tensor(\"patchy_layer_returnfullseq_160/transpose:0\", shape=(598, 30, ?), dtype=float32)\n",
      "M Tensor(\"patchy_layer_returnfullseq_160/GatherNd:0\", shape=(46, 60, 120, ?), dtype=float32)\n",
      "reshaped_nonzero_values Tensor(\"patchy_layer_returnfullseq_160/transpose_1:0\", shape=(?, 46, 60, 120), dtype=float32)\n",
      "PATCH_tensor1 after Tensor(\"patchy_layer_returnfullseq_160/transpose_1:0\", shape=(?, 46, 60, 120), dtype=float32)\n",
      "MPI first:: Tensor(\"patchy_layer_returnfullseq_160/Mul:0\", shape=(?, 46, 60, 120), dtype=float32)\n",
      "MPI before reansversal Tensor(\"patchy_layer_returnfullseq_160/leaky_re_lu_753/LeakyRelu:0\", shape=(?, 46, 60), dtype=float32)\n",
      "self.add_residual True\n",
      "self.nb_sequences -1\n",
      "MPI after reshape Tensor(\"patchy_layer_returnfullseq_160/leaky_re_lu_753/LeakyRelu:0\", shape=(?, 46, 60), dtype=float32)\n",
      "self.vector_size 598\n",
      "MPI after repeating Tensor(\"patchy_layer_returnfullseq_160/repeat/Reshape:0\", shape=(?, ?, ?), dtype=float32)\n",
      "y Tensor(\"patchy_layer_returnfullseq_160/transpose_2:0\", shape=(598, 30, ?), dtype=float32)\n",
      "M Tensor(\"patchy_layer_returnfullseq_160/GatherNd_1:0\", shape=(598, 9, 120, ?), dtype=float32)\n",
      "reshaped_nonzero_values Tensor(\"patchy_layer_returnfullseq_160/transpose_3:0\", shape=(?, 598, 9, 120), dtype=float32)\n",
      "PATCH_tensor_ontop Tensor(\"patchy_layer_returnfullseq_160/transpose_3:0\", shape=(?, 598, 9, 120), dtype=float32)\n",
      "W_MULT_ontop Tensor(\"patchy_layer_returnfullseq_160/Tile_2:0\", shape=(?, 598, 9, 120), dtype=float32)\n",
      "MPI_ontop Tensor(\"patchy_layer_returnfullseq_160/Mul_1:0\", shape=(?, 598, 9, 120), dtype=float32)\n",
      "MPI_ontop at end Tensor(\"patchy_layer_returnfullseq_160/leaky_re_lu_754/LeakyRelu:0\", shape=(?, 598, 9), dtype=float32)\n",
      "MPI after concat Tensor(\"patchy_layer_returnfullseq_160/concat:0\", shape=(?, 598, ?), dtype=float32)\n",
      "coords.shape (200, 40, 2)\n",
      "y Tensor(\"patchy_layer_cnntop_last_23/transpose:0\", shape=(598, 10, ?), dtype=float32)\n",
      "M Tensor(\"patchy_layer_cnntop_last_23/GatherNd:0\", shape=(200, 40, ?), dtype=float32)\n",
      "reshaped_nonzero_values Tensor(\"patchy_layer_cnntop_last_23/transpose_1:0\", shape=(?, 200, 40), dtype=float32)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-44da830c22a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-44da830c22a9>\u001b[0m in \u001b[0;36mget_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0mgru_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgru_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTimeDistributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgru_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m                                          \u001b[0;34m'You can build it manually via: '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m                                          '`layer.build(batch_input_shape)`')\n\u001b[0;32m--> 431\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/layers/wrappers.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInputSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0mchild_input_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from igloo1d import IGLOO_RETURNFULLSEQ,IGLOO\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "from keras_generator_final import Mygenerator\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import multiply, UpSampling1D,Add,Masking,Flatten,Concatenate,LeakyReLU,PReLU,Input,LSTM, core, Bidirectional, CuDNNLSTM, CuDNNGRU,Reshape,Lambda,Permute,TimeDistributed,RepeatVector,ConvLSTM2D,Conv3D,Dense,UpSampling3D,Embedding, SpatialDropout1D,GRU,Add,Activation,multiply\n",
    "from keras.layers.core import Dropout, Lambda\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose,UpSampling2D,AtrousConvolution2D,Conv1D,SeparableConv2D,SeparableConv1D\n",
    "from keras.layers.pooling import MaxPooling2D,MaxPooling3D,MaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import losses\n",
    "from keras import initializers\n",
    "from keras.regularizers import l2\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import load_model as LM\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.backend import squeeze,sum\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.constraints import maxnorm\n",
    "from keras.activations import tanh,softmax\n",
    "\n",
    "input_shape=(599,2736)\n",
    " \n",
    "nb_patches=200 \n",
    "nb_patches_FULL=60 \n",
    "nb_patches_vertical=5 \n",
    " \n",
    "patch_size=4 \n",
    "mDR=0.45 \n",
    "MAXPOOL_size=1 \n",
    " \n",
    "CONV1D_dim=10 \n",
    "nb_stacks=1 \n",
    "nb_stacks_full=1 \n",
    " \n",
    "igloo_l2reg=0.01 \n",
    " \n",
    "C1D_K=54 \n",
    " \n",
    "Conv1D_dim_full_seq=30 \n",
    " \n",
    "stretch_factor=13       #13 \n",
    " \n",
    "add_residual=True \n",
    "add_residual_vertical=True \n",
    "build_backbone=False \n",
    " \n",
    "padding=\"causal\" \n",
    "\n",
    "learning_rate = 0.00051 ##change\n",
    "lstm_output_size = 300 ##change\n",
    "vocab_size = 2536 ## change\n",
    "lstm_layers = 3\n",
    "dropout_rate = 0.22\n",
    "\n",
    "h5f = h5py.File('embedding.h5', 'r')\n",
    "balloon = h5f['embedding_dataset'][:]\n",
    "h5f.close()\n",
    " \n",
    " \n",
    "def get_model(): \n",
    " \n",
    "    CONC=[] \n",
    "    IGLOO_V=[] \n",
    " \n",
    "    inin = Input(shape=input_shape, name='input')\n",
    "    #inin= Lambda(lambda q: q[:,1:,:]) (inin) ##theinin\n",
    "\n",
    "    a=Conv1D(40,2,padding=\"causal\")(inin)\n",
    "    b=Conv1D(40,4,padding=\"causal\")(inin)\n",
    "    c=Conv1D(40,8,padding=\"causal\")(inin)\n",
    "\n",
    "    x=Concatenate(axis=-1)([a,b,c])\n",
    "    x=Activation(\"relu\")(x)\n",
    "    x= BatchNormalization(axis=-1)(x)\n",
    "\n",
    "    a=Conv1D(40,2,padding=\"causal\")(x)\n",
    "    b=Conv1D(40,4, padding=\"causal\")(x)\n",
    "    c=Conv1D(40,8, padding=\"causal\")(x)\n",
    "\n",
    "    x=Concatenate(axis=-1)([a,b,c])\n",
    "    x=Activation(\"relu\")(x)\n",
    "    x= BatchNormalization(axis=-1)(x)\n",
    "\n",
    "    a=Conv1D(40,2,padding=\"causal\")(x)\n",
    "    b=Conv1D(40,4,padding=\"causal\")(x)\n",
    "    c=Conv1D(40,8, padding=\"causal\")(x)\n",
    "\n",
    "    x=Concatenate(axis=-1)([a,b,c])\n",
    "    x=Activation(\"relu\")(x)\n",
    "    x= BatchNormalization(axis=-1)(x)\n",
    "    \n",
    "    x=Lambda(lambda q: q[:,1:,:]) (x) ##theinin\n",
    " \n",
    "    x=Conv1D(64, 1,strides=1,padding=padding)  (x) \n",
    "    x = BatchNormalization(axis=-1) (x) \n",
    "    x = Activation(\"relu\") (x) \n",
    "    x = SpatialDropout1D(mDR) (x)\n",
    "\n",
    "    IGLOO_V.append(IGLOO_RETURNFULLSEQ(x,nb_patches_FULL,Conv1D_dim_full_seq,patch_size=patch_size,padding_style=padding,stretch_factor=stretch_factor,l2reg=igloo_l2reg,\n",
    "                                      add_residual=add_residual,nb_stacks=nb_stacks_full,build_backbone=build_backbone)) \n",
    " \n",
    "\n",
    "    CONC.append(IGLOO_V[0]) \n",
    " \n",
    "    for kk in range(5): \n",
    " \n",
    "        x=Conv1D(C1D_K, 1,strides=1,padding=padding)  (CONC[kk]) \n",
    "        x = BatchNormalization(axis=-1) (x) \n",
    "        x = Activation(\"relu\") (x) \n",
    "        x = SpatialDropout1D(mDR) (x) \n",
    " \n",
    "        IGLOO_V.append(IGLOO_RETURNFULLSEQ(x,nb_patches_FULL,Conv1D_dim_full_seq,patch_size=patch_size,padding_style=padding,stretch_factor=stretch_factor,l2reg=igloo_l2reg,\n",
    "                                           add_residual=add_residual,nb_stacks=nb_stacks_full,build_backbone=build_backbone)) \n",
    " \n",
    " \n",
    "        ###second residual connection \n",
    "        co=Add() ([IGLOO_V[kk+1],CONC[kk]]) \n",
    "        CONC.append(Activation(\"relu\") (co)) \n",
    " \n",
    " \n",
    "    x=Conv1D(C1D_K, 1,strides=1,padding=padding)  (CONC[-1]) \n",
    "    x = BatchNormalization(axis=-1) (x) \n",
    "    x = Activation(\"relu\") (x) \n",
    "    x = SpatialDropout1D(mDR) (x) \n",
    " \n",
    "    y=IGLOO(x,nb_patches,CONV1D_dim,patch_size=patch_size,return_sequences=False,l2reg=igloo_l2reg,padding_style=padding,nb_stacks=nb_stacks,DR=mDR,max_pooling_kernel=MAXPOOL_size) \n",
    " \n",
    " \n",
    "    y=Dense(64,activation='relu') (y) \n",
    "    y=Dropout(0.4) (y)\n",
    "    output_1=Dense(1,activation='softmax') (y)\n",
    "\n",
    "    word_input = Input(shape=(9,), name='decoder_input')\n",
    "    \n",
    "\n",
    "    word_embedding=Embedding(input_dim=1149, output_dim=500, name='word_embedding',trainable=False,weights=[balloon],input_length=9)\n",
    "    embedded_word=word_embedding(word_input)\n",
    "\n",
    "\n",
    "    input_=embedded_word\n",
    "    \n",
    "\n",
    "    #input_ = BatchNormalization(axis=-1)(input_)\n",
    "    gru_out=GRU(900, activation='tanh', recurrent_activation='sigmoid', \n",
    "    dropout=0.22,return_sequences=True, return_state=False,unroll=False,reset_after=True)(input_)\n",
    "    \n",
    "    input_=gru_out\n",
    "    \n",
    "    input_ = BatchNormalization(axis=-1)(input_)\n",
    "    gru_out=GRU(1000, activation='tanh', recurrent_activation='sigmoid', \n",
    "    dropout=0.22,return_sequences=True, return_state=False,unroll=False,reset_after=True)(input_)\n",
    "    input_ = gru_out\n",
    "    \n",
    "    features=Permute((2,1))(x)\n",
    " \n",
    "    part1=Dense(1000)(features)\n",
    "    gru_out=Permute((2,1))(gru_out)\n",
    "    \n",
    "    shape= K.int_shape(features)  \n",
    "    \n",
    "    part2=Dense(shape[1])(gru_out)\n",
    "    part2=Permute((2,1))(part2)\n",
    "    part3= Add()([part1,part2])\n",
    "    \n",
    "    score = Activation(\"tanh\")(part3)\n",
    "    part4= Dense(1)(score)\n",
    "    \n",
    "    attention_weights=Lambda(lambda x: softmax(x,axis=1))(part4)\n",
    "    \n",
    "    context_vector=multiply([attention_weights,features])\n",
    "    context_vector=Lambda(lambda x: K.sum(x,axis=1))(context_vector)\n",
    "    \n",
    "    context_vector_mod=Dense(600)(context_vector)\n",
    "    context_vector_mod = Lambda(lambda x: K.expand_dims(x, -1))(context_vector_mod)\n",
    "    context_vector_mod=Permute((2,1))(context_vector_mod)\n",
    "    \n",
    "    gru_out_mod=Dense(600)(gru_out)\n",
    "\n",
    "    \n",
    "    input_=Concatenate(axis=1)([context_vector_mod, gru_out_mod])\n",
    "    input_=Activation(\"tanh\")(input_)\n",
    "\n",
    "\n",
    "    input_ = BatchNormalization(axis=-1)(input_)\n",
    "    gru_out=GRU(700, activation='tanh', recurrent_activation='sigmoid', \n",
    "    dropout=0.22,return_sequences=True, return_state=False,unroll=False,reset_after=True)(input_)\n",
    "    input_= gru_out\n",
    "\n",
    "\n",
    "    gru_out=Flatten()(gru_out)\n",
    "    sequence_output = TimeDistributed(Dense(units=vocab_size))(gru_out)\n",
    "\n",
    " \n",
    "    opt = optimizers.Adam(lr=0.001, clipnorm=1.0, decay=0.001) \n",
    "    model = Model(inputs=[inin,word_input],outputs=[output_1,sequence_output]) \n",
    " \n",
    "    model.compile(loss=['binary_crossentropy','categorical_crossentropy'],optimizer=opt, metrics=['accuracy']) \n",
    " \n",
    "    model.fit_generator(Mygenerator(12),epochs=300)\n",
    "    \n",
    "    return model \n",
    " \n",
    " \n",
    " \n",
    " \n",
    "model=get_model() \n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from igloo1d import IGLOO_RETURNFULLSEQ,IGLOO\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import multiply, UpSampling1D,Add,Masking,Flatten,Concatenate,LeakyReLU,PReLU,Input,LSTM, core, Bidirectional, CuDNNLSTM, CuDNNGRU,Reshape,Lambda,Permute,TimeDistributed,RepeatVector,ConvLSTM2D,Conv3D,Dense,UpSampling3D,Embedding, SpatialDropout1D,GRU,Add,Activation,multiply\n",
    "from keras.layers.core import Dropout, Lambda\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose,UpSampling2D,AtrousConvolution2D,Conv1D,SeparableConv2D,SeparableConv1D\n",
    "from keras.layers.pooling import MaxPooling2D,MaxPooling3D,MaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import losses\n",
    "from keras import initializers\n",
    "from keras.regularizers import l2\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import load_model as LM\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.backend import squeeze,sum\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.constraints import maxnorm\n",
    "from keras.activations import tanh,softmax\n",
    "from keras_generator_final import Mygenerator\n",
    "from keras.models import load_model\n",
    "\n",
    "input_shape=(599,2736)\n",
    " \n",
    "nb_patches=200 \n",
    "nb_patches_FULL=60 \n",
    "nb_patches_vertical=5 \n",
    " \n",
    "patch_size=4 \n",
    "mDR=0.45 \n",
    "MAXPOOL_size=1 \n",
    " \n",
    "CONV1D_dim=10 \n",
    "nb_stacks=1 \n",
    "nb_stacks_full=1 \n",
    " \n",
    "igloo_l2reg=0.01 \n",
    " \n",
    "C1D_K=54 \n",
    " \n",
    "Conv1D_dim_full_seq=30 \n",
    " \n",
    "stretch_factor=13       #13 \n",
    " \n",
    "add_residual=True \n",
    "add_residual_vertical=True \n",
    "build_backbone=False \n",
    " \n",
    "padding=\"causal\" \n",
    "\n",
    "learning_rate = 0.01 ##change\n",
    "\n",
    "vocab_size = 1149 \n",
    "lstm_layers = 3\n",
    "dropout_rate = 0.22\n",
    "\n",
    "h5f = h5py.File('embedding.h5', 'r')\n",
    "balloony = h5f['embedding_dataset'][:]\n",
    "h5f.close()\n",
    "\n",
    "def sparse_cross_entropy(y_true, y_pred):\n",
    "    \n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true,\n",
    "                                                          logits=y_pred)\n",
    "\n",
    "    loss_mean = tf.reduce_mean(loss)\n",
    "\n",
    "    return loss_mean\n",
    " \n",
    " \n",
    "def get_model(): \n",
    " \n",
    "    CONC=[] \n",
    "    IGLOO_V=[] \n",
    " \n",
    "    inin = Input(shape=input_shape, name='input')\n",
    "    \n",
    "    #inin=Lambda(lambda q: q[:,1:,:]) (inin) \n",
    "\n",
    "    a=Conv1D(40,2,padding=\"causal\")(inin)\n",
    "    b=Conv1D(40,4,padding=\"causal\")(inin)\n",
    "    c=Conv1D(40,8,padding=\"causal\")(inin)\n",
    "\n",
    "    x=Concatenate(axis=-1)([a,b,c])\n",
    "    x=Activation(\"relu\")(x)\n",
    "    x= BatchNormalization(axis=-1)(x)\n",
    "\n",
    "    a=Conv1D(40,2,padding=\"causal\")(x)\n",
    "    b=Conv1D(40,4, padding=\"causal\")(x)\n",
    "    c=Conv1D(40,8, padding=\"causal\")(x)\n",
    "\n",
    "    x=Concatenate(axis=-1)([a,b,c])\n",
    "    x=Activation(\"relu\")(x)\n",
    "    x= BatchNormalization(axis=-1)(x)\n",
    "\n",
    "    a=Conv1D(40,2,padding=\"causal\")(x)\n",
    "    b=Conv1D(40,4,padding=\"causal\")(x)\n",
    "    c=Conv1D(40,8, padding=\"causal\")(x)\n",
    "\n",
    "    x=Concatenate(axis=-1)([a,b,c])\n",
    "    x=Activation(\"relu\")(x)\n",
    "    x= BatchNormalization(axis=-1)(x)\n",
    "    \n",
    "    inin=Lambda(lambda q: q[:,1:,:]) (x)\n",
    " \n",
    "    x=Conv1D(64, 1,strides=1,padding=padding)  (x) \n",
    "    x = BatchNormalization(axis=-1) (x) \n",
    "    x = Activation(\"relu\") (x) \n",
    "    x = SpatialDropout1D(mDR) (x)\n",
    "\n",
    "    IGLOO_V.append(IGLOO_RETURNFULLSEQ(x,nb_patches_FULL,Conv1D_dim_full_seq,patch_size=patch_size,padding_style=padding,stretch_factor=stretch_factor,l2reg=igloo_l2reg,\n",
    "                                      add_residual=add_residual,nb_stacks=nb_stacks_full,build_backbone=build_backbone)) \n",
    " \n",
    "\n",
    "    CONC.append(IGLOO_V[0]) \n",
    " \n",
    "    for kk in range(5): \n",
    " \n",
    "        x=Conv1D(C1D_K, 1,strides=1,padding=padding)  (CONC[kk]) \n",
    "        x = BatchNormalization(axis=-1) (x) \n",
    "        x = Activation(\"relu\") (x) \n",
    "        x = SpatialDropout1D(mDR) (x) \n",
    " \n",
    "        IGLOO_V.append(IGLOO_RETURNFULLSEQ(x,nb_patches_FULL,Conv1D_dim_full_seq,patch_size=patch_size,padding_style=padding,stretch_factor=stretch_factor,l2reg=igloo_l2reg,\n",
    "                                           add_residual=add_residual,nb_stacks=nb_stacks_full,build_backbone=build_backbone)) \n",
    " \n",
    " \n",
    "        ###second residual connection \n",
    "        co=Add() ([IGLOO_V[kk+1],CONC[kk]]) \n",
    "        CONC.append(Activation(\"relu\") (co)) \n",
    " \n",
    " \n",
    "    x=Conv1D(C1D_K, 1,strides=1,padding=padding)  (CONC[-1]) \n",
    "    x = BatchNormalization(axis=-1) (x) \n",
    "    x = Activation(\"relu\") (x) \n",
    "    x = SpatialDropout1D(mDR) (x) \n",
    " \n",
    "    y=IGLOO(x,nb_patches,CONV1D_dim,patch_size=patch_size,return_sequences=False,l2reg=igloo_l2reg,padding_style=padding,nb_stacks=nb_stacks,DR=mDR,max_pooling_kernel=MAXPOOL_size) \n",
    " \n",
    " \n",
    "    y=Dense(64,activation='relu') (y) \n",
    "    y=Dropout(0.4) (y)\n",
    "    output_1=Dense(1,activation='softmax') (y)\n",
    "\n",
    "    word_input = Input(shape=(29,), name='decoder_input')\n",
    "    \n",
    " \n",
    "    embedded_word=Embedding(input_dim=1149, output_dim=500, name='word_embedding',trainable=False,weights=[balloony],input_length=29)(word_input)\n",
    "   \n",
    "\n",
    "\n",
    "    input_=embedded_word\n",
    "    \n",
    "\n",
    "    #input_ = BatchNormalization(axis=-1)(input_)\n",
    "    gru_out=GRU(900, activation='tanh', recurrent_activation='sigmoid', \n",
    "    dropout=0.22,return_sequences=True, return_state=False,unroll=False,reset_after=True)(input_)\n",
    "    \n",
    "    input_=gru_out\n",
    "    \n",
    "    input_ = BatchNormalization(axis=-1)(input_)\n",
    "    gru_out=GRU(1000, activation='tanh', recurrent_activation='sigmoid', \n",
    "    dropout=0.22,return_sequences=True, return_state=False,unroll=False,reset_after=True)(input_)\n",
    "    input_ = gru_out\n",
    "    \n",
    "    features=Permute((2,1))(x)\n",
    " \n",
    "    part1=Dense(1000)(features)\n",
    "    gru_out=Permute((2,1))(gru_out)\n",
    "    \n",
    "    shape= K.int_shape(features)  \n",
    "    \n",
    "    part2=Dense(shape[1])(gru_out)\n",
    "    part2=Permute((2,1))(part2)\n",
    "    part3= Add()([part1,part2])\n",
    "    \n",
    "    score = Activation(\"tanh\")(part3)\n",
    "    part4= Dense(1)(score)\n",
    "    \n",
    "    attention_weights=Lambda(lambda x: softmax(x,axis=1))(part4)\n",
    "    \n",
    "    context_vector=multiply([attention_weights,features])\n",
    "    context_vector=Lambda(lambda x: K.sum(x,axis=1))(context_vector)\n",
    "    \n",
    "    context_vector_mod=Dense(600)(context_vector)\n",
    "    context_vector_mod = Lambda(lambda x: K.expand_dims(x, -1))(context_vector_mod)\n",
    "    context_vector_mod=Permute((2,1))(context_vector_mod)\n",
    "    \n",
    "    gru_out_mod=Dense(600)(gru_out)\n",
    "\n",
    "    \n",
    "    input_=Concatenate(axis=1)([context_vector_mod, gru_out_mod])\n",
    "    input_=Activation(\"tanh\")(input_)\n",
    "\n",
    "\n",
    "    input_ = BatchNormalization(axis=-1)(input_)\n",
    "    gru_out=GRU(700, activation='tanh', recurrent_activation='sigmoid', \n",
    "    dropout=0.22,return_sequences=True, return_state=False,unroll=False,reset_after=True)(input_)\n",
    "    input_= gru_out\n",
    "\n",
    "\n",
    "    \n",
    "    sequence_output = TimeDistributed(Dense(units=vocab_size))(gru_out)\n",
    "\n",
    " \n",
    "    opt = optimizers.Adam(lr=0.001, clipnorm=1.0, decay=0.001) \n",
    "    model = Model(inputs=[inin,word_input],outputs=[output_1,sequence_output]) \n",
    "    \n",
    "\n",
    " \n",
    "    model.compile(loss=['binary_crossentropy','sparse_categorical_crossentropy'],optimizer=opt, metrics=['accuracy'],loss_weights=[100,1]) \n",
    " \n",
    "    #model.fit_generator(Mygenerator(12),epochs=300)\n",
    "    \n",
    "    #model.save('my_eeg_model_final.h5')\n",
    "    return model\n",
    "    #del model\n",
    "    \n",
    "\n",
    " \n",
    " \n",
    "model=get_model()\n",
    "print(model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
